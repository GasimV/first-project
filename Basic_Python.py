#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd


# In[40]:


hours = 285 // 60
minutes = 285 % 60
print(f'Общая продолжительность боевиков составляет {hours} часа {minutes} минут.')


# In[6]:


atlas = [  
    ['Франция','Париж'],  
    ['Россия','Москва'],  
    ['Китай','Пекин'],  
    ['Мексика','Мехико'],  
    ['Египет','Каир']  
]

geography = ['country', 'capital']

world_map = pd.DataFrame(data=atlas, columns=geography) 
world_map


# In[7]:


world_map.head()


# In[11]:


print(world_map.shape)
print(world_map.shape[0])


# In[12]:


world_map.info()


# In[4]:


movies_table = [
    ['Побег из Шоушенка', 'США', 1994, 'драма', 142, 9.111],
    ['Крёстный отец', 'США', 1972, 'драма, криминал', 175, 8.730],
    ['Тёмный рыцарь', 'США', 2008, 'фантастика, боевик, триллер', 152, 8.499],
    ['Список Шиндлера', 'США', 1993, 'драма', 195, 8.818],
    ['Властелин колец: Возвращение Короля', 'Новая Зеландия', 2003, 'фэнтези, приключения, драма', 201, 8.625],
    ['Криминальное чтиво', 'США', 1994, 'триллер, комедия, криминал', 154, 8.619],
    ['Хороший, плохой, злой', 'Италия', 1966, 'вестерн', 178, 8.521],
    ['Бойцовский клуб', 'США', 1999, 'триллер, драма, криминал', 139, 8.644],
    ['Харакири', 'Япония', 1962, 'драма, боевик, история', 133, 8.106],
    ['Сталкер', 'СССР', 1979, 'фантастика, драма, детектив', 163, 8.083],
    ['Иди и смотри', 'СССР', 1985, 'драма, военный', 136, 8.094]
]

columns = ['movie_name', 'country', 'year', 'genre', 'duration', 'rating'] # сохраняем названия будущих колонок в список

movies_df = pd.DataFrame(data=movies_table, columns=columns) 

print(movies_df)


# In[3]:


print(movies_df[movies_df['duration'] > 180]) 


# In[6]:


print(movies_df.columns)


# In[18]:


# Доступ к индексации открывает атрибут - df.loc[строка, столбец].

movies_df.loc[2, 'movie_name']


# In[20]:


# Срезы строк в таблицах похожи на срезы в списках. Через : вы указываете начало и конец среза и получаете все значения между 
# ними. Края диапазона при этом попадают в срез: запрос строк df.loc[2:4] вернёт строки с индексами 2, 3 и 4.

# Индексация колонок устроена так же. Но ещё можно обращаться к списку колонок, а не только их срезу.

# Срез слобца
movies_df.loc[:, 'movie_name']


# In[21]:


# срез нескольких столбцов по их названиям

movies_df.loc[: , ['movie_name', 'year']]


# In[22]:


# одна строка

movies_df.loc[1]


# In[25]:


# срез строк в заданном диапазоне и выбор определённого столбца

movies_df.loc[2:7, 'movie_name']


# In[26]:


# Морской бой

data = [[0,0,0,0,0,0,0,0,0,0],
        [0,'-','-','-',0,0,0,0,0,0],
        [0,'-','X','-',0,0,'X','X','X','X'],
        [0,'-','X','-',0,0,0,0,0,0],
        [0,'-','-','-',0,0,0,0,0,0],
        [0,0,'-',0,0,0,0,0,'X',0],
        [0,'-','X','X',0,0,0,0,0,0],
        [0,0,'-','-',0,0,0,0,0,0],
        [0,0,0,0,'-','X',0,0,0,0],
        [0,0,0,0,0,0,0,0,0,0]]

columns = ['А','Б','В','Г','Д','Е','Ж','З','И','К']

battle = pd.DataFrame(data = data, columns = columns)

print(battle) 


# In[27]:


# Оценим ситуацию на поле. Запросим столбец В — там уже началась атака на корабль:

print(battle.loc[:,'В'])


# In[28]:


# Атака уже началась в седьмой строке — то есть строке с индексом 6. Но корабль ещё не убит, а выстрелы в ячейки 
# сверху и снизу не попали в цель. Значит, нужно оценить расположение корабля по горизонтали. Запросим седьмую строку целиком:

print(battle.loc[6]) 


# In[29]:


# Корабль ориентирован по горизонтали. В него уже попали два выстрела — они пришлись на столбцы В и Г. 
# Оценим ситуацию вокруг корабля: посмотрим строки с 5 по 7:

print(battle.loc[5:7]) 


# In[30]:


# Очевидно, что следующий выстрел нужно сделать по координате с индексом 6 и названием колонки Д.

print(battle.loc[6, 'Д'])


# In[31]:


# Сокращённая запись при индексации: срез нескольких столбцов по их названиям
battle[['А', 'Г']]


# In[32]:


# срез нескольких строк подряд
battle[2:5]


# In[33]:


# Логическая индексация

# Этот сложный алгоритм в pandas можно записать одной строчкой. Сначала разберём фильтрацию.
# Найдём все строки, где в столбце "B" встречается 'X':

battle.loc[battle.loc[:,'В'] == 'X']   


# In[34]:


# Счётчик выглядит немногим сложнее. Посчитаем ячейки в "B" со значением 'X'. Сначала получим эти ячейки:

# Это последовательная фильтрация - когда 2 списка идут друг за другом и 2 условия обьединяются!
battle.loc[battle.loc[:,'В'] == 'X']['В']


# In[35]:


# Чтобы посчитать получившиеся ячейки, вызовем метод count():

print(battle.loc[battle.loc[:,'В'] == 'X']['В'].count())  


# In[36]:


# У логической индексации тоже бывает сокращённая запись. Пример выше можно записать следующим образом:

print(battle[battle['В'] == 'X']['В'].count()) 


# In[38]:


# Здесь можно использовать логические выражения с любыми логическими операциями: ==, !=, >, <, >=, <=, а также 
# функции-предикаты, которые возвращают логическое значение. 

movies_df[movies_df['year'] >= 2000]

# Чтобы применить несколько условий для фильтрации, сначала примените первое условие и сохраните результат. 
# Затем вызовите логическую индексацию со 2-м условием.


# In[43]:


# Индексация в "Series" Индексация Series действует так же, как и в датафреймах. 
# Главное отличие — отсутствие второй оси координат:

# Получаем Series из датафрейма
genre = movies_df['genre']

# Получаем ячейку из Series по единственной координате
print(genre)
print()
print(genre[2])


# При сокращённой записи, в таблице нельзя обратиться к отдельной строке, не указывая в то же время название колонки. 
# В Series колонка только одна, поэтому такой проблемы нет.


# In[48]:


# Логическая индексация в Series тоже работает. Она выглядит существенно проще, чем в датафреймах. 
# В Series достаточно одного логического условия и не нужно указывать на столбец, из которого индексация должна вернуть данные.

movies_df.loc[movies_df['year'] <= 1993]

# Когда для анализа данных из всей таблицы достаточно одного столбца, лучше извлечь его в отдельную переменную. 
# Тогда не придётся каждый раз указывать имя столбца. Несколько задач помогут в этом убедиться.


# # Предобработка данных <a id='dataprep'></a>

# In[8]:


# Assign new value to the Dictionary

thisdict = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}

print(thisdict["year"])

thisdict["year"] = 2018
print(thisdict["year"])


# In[22]:


# Rename DataFrame Columns Value

df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
print(df)

df.rename(columns={"A": "a", "B": "c"})


# In[ ]:


# Переименование столбцов: Задача 2/3

# df = pd.read_csv('music_log.csv')

# df = df.rename(columns={"  user_id": "user_id", "total play": "total_play", "Artist": "artist"})

# print(df)


# In[23]:


print(movies_df.columns)


# In[25]:


# Когда названия столбцов исправлены, можно обратиться к самим данным. Получим первые 10 строк:

print(movies_df.head(10))


# ## Обработка пропущенных значений

# In[27]:


# Выглядеть пропущенные значения тоже могут по-разному. Два основных типа пропусков:

    # Ожидаемые, None или NaN. Это специальные значения, которые указывают на пропуск. Между ними есть важное отличие:
        # None относится к типу NoneType;
        # NaN — к вещественным числам, float.
        # Из-за этого NaN можно использовать в математических операциях, не вызывая ошибку из-за несовместимых типов.
    
    # Странные. Разработчики могут выбрать любое другое обозначение для пропусков. Например: 0, '?', 'NN', 'n/a'. 
    # В лучшем случае об их решении вы узнаете из документации к данным. В худшем — придётся догадаться самостоятельно. 
    # Если какой-нибудь спецсимвол или число часто встречаются в таблице и этому нет внятного объяснения — высока вероятность, 
    # что так передаются пропущенные значения.
    
# Методы борьбы с пропусками покажем на примере датасета Всемирной организации здравоохранения. 
# Отчёт ВОЗ содержит статистику по заболеваемости холерой с 1989 по 2017 год:

# cholera = pd.read_csv('cholera.csv')
# print(cholera) 

# Документация к данным
# Для разных регионов (колонка region) и стран (country) указано общее число случаев заболевания холерой (total_cases), 
# в том числе завозные случаи (imported_cases) и смертельные (deaths). Всё это целые числа — данные приведены с точностью до 
# человека. Столбец case_fatality_rate содержит процент летальных исходов. Столбец notes — строки с примечаниями.


# Поиск пропущенных значений
# Специальный метод isna() найдёт все пропуски в таблице. Если значение отсутствует, он вернёт True, иначе — False.
# Чтобы посчитать пропуски, результат работы isna() передадим методу sum():

# print(cholera.isna().sum()) 


# Иногда встречается код, где вместо isna() вызывают isnull(). 
# Это действительно равносильные команды. Но хорошим тоном среди специалистов считается вызов isna().



# Обработка пропусков
# В зависимости от целей исследования с пропусками можно обойтись по-разному:

# Строки таблицы удаляют полностью, если они потеряли смысл из-за пропущенных значений. 
# Иногда приходится признать, что часть данных бесполезна из-за пропусков — тогда от таких строк остаётся только избавиться.
# Так, в таблице ВОЗ нет данных по Европе. Вся строка состоит из пропущенных значений. 
# Примечание сообщает, что c 1989 по 2017 год европейцы холерой не болели. Значит, строку можно удалить — это никак не 
# повлияет на результат.

# Иногда пропуски заполняют другими значениями. Это допустимо, когда пропущенные значения неважны для целей исследования, но 
# в тех же строках или столбцах остаются ценные данные. Например, в Африке зафиксировано 179835 случаев заболевания. 
# Но ни одного завозного случая: в колонке imported_cases стоит значение NaN. Если удалить всю строку из-за одного пропуска, 
# потеряются важные данные. Чтобы не лишиться строк с важными данными, заполним значения NaN в столбце imported_cases нулями.
# Для этого вызывают специальный метод fillna(). Он вернёт копию исходного столбца, заменяя все NaN на значение из аргумента:

# cholera['imported_cases'] = cholera['imported_cases'].fillna(0) 


# От оставшихся строк со значением NaN избавимся методом dropna(). Он удаляет строки, где встречается хотя бы один пропуск. 
# Через параметр subset методу можно передать список столбцов, в которых он должен искать пропуски.
# При этом может нарушиться порядок в нумерации строк.

# cholera = cholera.dropna(subset=['total_cases', 'deaths', 'case_fatality_rate' ]) 


# Другой аргумент dropna(), axis, задаёт оси таблицы. Так как датафрейм это двумерная структура, у него две оси: 
# строки и столбцы. Если этому аргументу присвоить значение 'columns', он удалит любой столбец с пропуском.

# cholera = cholera.dropna(axis='columns') 


# ## Обработка дубликатов

# In[47]:


# Поиск явных дубликатов

# Метод duplicated() ищет дубликаты. По умолчанию он признаёт дубликатами те строки, которые полностью повторяют уже 
# встречавшиеся в датасете. Метод возвращает Series со значением True для таких строк. Выглядит это так:

df = pd.DataFrame({
    'item': ['shirt', 'shirt', 'shirt', 'shirt', 'hat'],
    'style': ['classic', 'classic', 'casual', 'classic', 'casual'],
    'rating': [4, 4, 3.5, 4, 5]
})

print(df, '\n')
print(df.duplicated())


# In[48]:


# Чтобы посчитать такие строки, результат метода передают функции sum():

print(df.duplicated().sum()) 


# In[49]:


# Чтобы получить таблицу с такими дубликатами, применим логическую индексацию. 
# От duplicated() мы получили Series с логическими значениями — теперь запросим из исходной таблицы строки, 
# которым соответствует True в этом Series. Выведем на экран первые несколько строк с результатами:

duplicated_df = df[df.duplicated()].head() #результат — датафрейм с дубликатами
print(duplicated_df) 


# In[50]:


# Удаление явных дубликатов

# Чтобы избавиться от таких дубликатов, вызовем метод drop_duplicates():

df = df.drop_duplicates()
print(df)

# Здесь проявится важное свойство датафреймов. Когда из них удаляют строки, нарушается порядок индексов. 
# Взгляните на первые пять строчек после удаления дубликатов:


# In[51]:


# После удаления строчек лучше обновить индексацию: чтобы в ней не осталось пропусков. Для этого вызовем метод reset_index(). 
# Он создаст новый датафрейм, где:

   # индексы исходного датафрейма станут новой колонкой с названием index;
   # все строки получат обычные индексы, уже без пропусков.

df = df.drop_duplicates().reset_index()
print(df)


# In[52]:


# Можно и не создавать столбец index. Для этого у метода reset_index() изменим специальный параметр:

df = df.drop_duplicates().reset_index(drop=True)


# In[53]:


# Поиск неявных дубликатов

# Когда в колонке с genre один и тот же жанр называется то «джаз», то “jazz”, — это неявные дубликаты. Найти их сложнее, но 
# всё же возможно. Неявные дубликаты ищут методом unique(). Он возвращает перечень уникальных значений в столбце:

print(df['style'].unique())

# или если список длинный - "print(sorted(df['genre_name'].unique()))"


# In[54]:


# Среди уникальных значений ищут неявные дубликаты:

    # альтернативные написания одного и того же значения. Например: 'jazz' и 'джаз'.
    # ошибочные написания. Например 'jazzz' вместо 'jazz'.

# Опробуем этот метод на небольшом датасете.
# В 2018 году рейтинг первой ракетки мира, по версии ATP, обновлялся 17 раз. Историю этих изменений сохраним в таблицу:

rating = ['date','name','points']
players = [
    ['2018.01.01',    'Рафаэль Надаль',10645],
    ['2018.01.08',    'Рафаэль Надаль',    10600],
    ['2018.01.29',    'Рафаэль Надаль',    9760],
    ['2018.02.19',    'Роджер Федерер',    10105],    
    ['2018.03.05',    'Roger Fderer',    10060],
    ['2018.03.19',    'Roger Fdrer',    9660],
    ['2018.04.02',    'Рафаэль Надаль',    8770],
    ['2018.06.18',    'Roger Federer',    8920],
    ['2018.06.25',    'Рафаэль Надаль',    8770],
    ['2018.07.16',    'Рафаэль Надаль',    9310],
    ['2018.08.13',    'Рафаэль Надаль',    10220],
    ['2018.08.20',    'Рафаэль Надаль',    10040],
    ['2018.09.10',    'Рафаэль Надаль',    8760],
    ['2018.10.08',    'Рафаэль Надаль',    8260],
    ['2018.10.15',    'Рафаэль Надаль',    7660],
    ['2018.11.05',    'Новак Джокович',    8045],
    ['2018.11.19',    'Новак Джокович',    9045]
]
tennis = pd.DataFrame(data=players, columns=rating)
print(tennis) 


# In[55]:


# Вызовем метод unique() для столбца с именами теннисистов:

print(tennis['name'].unique())

# Результатом стали шесть имён. Хотя нет, не шесть. Кроме имени Роджера Федерера на русском языке, вернулось его же имя 
# латиницей в трёх вариантах: правильном и с опечаткой. Это и есть неявные дубликаты.


# In[56]:


# Удаление неявных дубликатов

# Неправильные и альтернативные написания значений исправляют методом replace(). 
# В первом аргументе ему передают нежелательное значение из таблицы. 
# Во втором — новое значение, которое должно заменить дубликат:

tennis['name'] = tennis['name'].replace('Roger Federer', 'Роджер Федерер')
tennis['name'] = tennis['name'].replace('Roger Fderer', 'Роджер Федерер')
tennis['name'] = tennis['name'].replace('Roger Fdrer', 'Роджер Федерер')
 
print(tennis) 


# In[57]:


# Вызов метода replace() пришлось повторить трижды. Этого можно избежать, если поместить неявные дубликаты в список и 
# передать его в качестве первого аргумента методу replace().

duplicates = ['Roger Fderer', 'Roger Fdrer', 'Roger Federer'] # список неправильных имён
name = 'Роджер Федерер' # правильное имя
tennis['name'] = tennis['name'].replace(duplicates, name) # замена всех значений из duplicates на name

print(tennis) # датафрейм изменился, неявные дубликаты устранены 


# In[59]:


# Другое решение — создать специальную собственную функцию, которая заменит значения из списка. 
# Например, запишем функцию, которая принимает два аргумента:

    # список неправильных значений,
    # правильное значение.

# Функция заменит все неправильные значения на правильные для выбранного столбца:

# на вход функции подаются список неправильных значений и строка с правильным значением
def replace_wrong_values(wrong_values, correct_value): 
    for wrong_value in wrong_values: # перебираем неправильные имена
        tennis['name'] = tennis['name'].replace(wrong_value, correct_value) 
        # и для каждого неправильного имени вызываем метод replace()

duplicates = ['Roger Fderer', 'Roger Fdrer', 'Roger Federer'] # список неправильных имён
name = 'Роджер Федерер' # правильное имя

replace_wrong_values(duplicates, name) # вызов функции, replace() внутри будет вызван 3 раза

print(tennis) # датафрейм изменился, неявные дубликаты устранены 


# In[ ]:


# импортируйте библиотеку pandas
import pandas as pd
# считайте csv-файл 'music_log.csv' в переменную df
df = pd.read_csv('music_log.csv')

# переименуйте названия столбцов df
df = df.rename(columns={'  user_id': 'user_id', 'total play': 'total_play', 'Artist': 'artist'})


# объявите список columns_to_replace с названиями столбцов track, artist, genre
# заполните отсутствующие значения столбцов из списка columns_to_replace значением 'unknown' в цикле
columns_to_replace = ['track', 'artist', 'genre']
for column in columns_to_replace:
    df[column] = df[column].fillna('unknown') 

# удалите строки-дубликаты из датафрейма df
df = df.drop_duplicates().reset_index(drop=True)

# выведите на экран первые 20 строчек обновлённого набора данных df
print(df.head(20))


# # Анализ данных и оформление результатов

# ## Задача: обновление Яндекс Музыки

# In[2]:


# Раньше Яндекс.Музыка и Яндекс.Радио были двумя независимыми сервисами. Однажды разработчики задумались об их объединении и 
# даже подготовили такую версию приложения. Но масштабное обновление — всегда риск. Пользователи уже привыкли к приложению, и 
# обновление может только раздражать. Прежде чем запустить новую версию, в Яндексе провели исследование на небольшой группе 
# пользователей. В этой теме вы повторите часть этого исследования и оцените влияние объединения сервисов на пользователей.
# Компании по-разному оценивают реакцию пользователя на обновления. Но чаще всего оценивают пять метрик HEART модели:

    # Happiness — счастье. Например, высокая оценка приложения от пользователя;
    # Engagement — вовлечённость. Например, количество действий, которые пользователь совершает за единицу времени.
    # Adoption — принятие сервиса. Например, нового обновления.
    # Retention — удержание. Например, сколько раз пользователь открывал приложение за единицу времени.
    # Task success — буквально, «успех задач». Эта метрика показывает, оправдываются ли ожидания пользователя от сервиса.

# Вы оцените вовлечённость пользователей: для этого оцените среднее время прослушивания трека. 
# Чем выше этот показатель, тем выше вовлечённость пользователя.

# Цель исследования
# Новую версию приложения, объединяющую Музыку и Радио, протестировали на небольшой группе пользователей. 
# Результаты сохранили в csv-файл.
# Ваша задача: найти значение метрики engagement и посмотреть, как оно поменялось по сравнению со старой версией приложения.


# ## Начало исследования

# ### Обзор данных

# In[3]:


# Исходные данные хранятся в файле .csv. Чтобы исследовать их средствами pandas, эту библиотеку нужно импортировать:
# Тогда станет доступен метод для чтения csv-файлов — read_csv(). 

# Прочитаем файл по адресу из аргумента, преобразуем его в таблицу и сохраним в переменную df:
# df = pd.read_csv('music_log.csv') 

# Теперь с данными можно работать. Посмотрим первые 15 строчек таблицы:
# print(df.head(15))

# Нужно убедиться в том, что данные прошли предобработку. В них не должно быть пропусков и дубликатов.
# Пропущенные значения выявляет метод isna(), а подсчитывает — метод sum():

# print(df.isna().sum()) 

# Дубликаты, повторяющиеся строки, выявляют методом duplicated() и подсчитывают тем же sum():
# print(df.duplicated().sum()) 

# Если обе проверки не выявили проблемы в данных, значит, они готовы для анализа.


# ## Группировка данных

# In[4]:


# Часто анализ данных начинают с разделения их на группы по какому-нибудь признаку. Эта операция называется группировкой данных.
# Она помогает изучить материал более подробно, чтобы затем перейти к поиску взаимосвязей между отдельными группами.
# Группировка оправдана, если данные чётко делятся по значимому признаку, а полученные группы близки к теме задачи. 
# Например, когда есть данные обо всех покупках в супермаркете, можно смело заниматься группировкой. 
# Так можно установить время наплыва покупателей и решить проблему пиковых нагрузок. 
# Или посчитать средний чек — обычно для магазинов это ключевая метрика.
# Стадии группировки выражают формулой split-apply-combine:

    # разделить, split — сначала данные разбивают на группы по определённому критерию;
    # применить, apply — затем к каждой группе применяют методы вычисления, например: считают элементы группы методом count() 
    # или их суммы методом sum();
    # объединить, combine — наконец, результаты сводят в новую структуру данных, таблицу или Series.

# Это стандартные этапы исследования — поэтому для них в pandas уже заготовлены специальные методы. 
# Рассмотрим их на примере анализа данных об экзопланетах — планетах за пределами Солнечной системы. 
# Орбитальные обсерватории засекли уже тысячи таких небесных тел. Их выявляют на снимках космических телескопов наши коллеги, 
# аналитики данных. Покажем, как среди них находят планеты, похожие на Землю.
# Таблица с данными по нескольким тысячам экзопланет сохранена в переменной exoplanet. Посмотрите на первые 30 строк таблицы:

# print(exoplanet.head(30))

# Документация к данным:

    # name — название экзопланеты;
    # mass — масса в массах планеты Юпитер;
    # radius — радиус, пересчитанный в радиусах Земли;
    # discovered — год открытия экзопланеты.

# Источник: каталог экзопланет на портале http://exoplanet.eu/catalog/

# Сначала исследуем, как менялось исследование новых планет во времени. Для этого сгруппируем данные по годам — 
# колонке discovered. Принцип split-apply-combine для этой задачи можно описать такой схемой:

#    1. Сначала данные делят по группам, где каждая группа — это год.
#    2. Потом метод count() считает элементы каждой группы.
#    3. Результатом станет новая таблица, где каждая строка содержит год и число открытых за этот год экзопланет.


# В pandas данные группируют методом groupby(), который:

#   Принимает в качестве аргумента название столбца, по которому нужно группировать. 
#     В случае с делением экзопланет это год открытия.
#   Возвращает объект особого типа — DataFrameGroupBy. Это сгруппированные данные. 
#     Если применить к ним какой-нибудь метод pandas, они станут новой структурой данных типа DataFrame или Series.

# Посчитаем сгруппированные по годам экзопланеты методом count():

# print(exoplanet.groupby('discovered').count())


# Результат выполнения кода exoplanet.groupby('discovered').count() — это уже новая таблица, датафрейм. 
# По таблице сразу заметна тенденция: количество открытых экзопланет растёт почти ежегодно.

# Если нужно сравнить наблюдения по одному показателю, метод применяют к DataFrameGroupBy с указанием на один столбец. 
# Например, чтобы найти похожие на Землю планеты, нужно найти планеты с близким по величине радиусом. Сначала посчитаем планеты,
# для которых радиус вообще известен:

# exo_number = exoplanet.groupby('discovered')['radius'].count()
# print(exo_number)


# Получили Series, где по годам открытия расписано количество экзопланет, для которых удалось установить радиус.
# Посмотрим, как меняется средний радиус открытых экзопланет год от года. Для этого надо сложить радиусы планет, 
# открытых за определённый год, и поделить их на количество планет — которое мы нашли на прошлом шаге.
# Сумма радиусов считается методом sum():

# exo_radius_sum = exoplanet.groupby('discovered')['radius'].sum()
# print(exo_radius_sum)


# Объекты Series можно делить друг на друга. Это позволит разделить перечень сумм радиусов на перечень количеств экзопланет 
# без перебора в цикле:

# exo_radius_mean = exo_radius_sum/exo_number
# print(exo_radius_mean) 


# Точность приборов растёт, и новые экзопланеты по размерам всё ближе к Земле. 
# За 24 года средний радиус обнаруженных планет уменьшился втрое.


# In[5]:


# Сгруппируйте данные по user_id, а колонку genre_name выберите как показатель для сравнения. 
# Результат сохраните в переменной track_grouping.

# track_grouping = df.groupby('user_id')['genre_name']


# ## Сортировка данных

# In[3]:


# Поиск необычного в группе — что среди планет, что среди пользователей — это прежде всего поиск чемпионов: объектов с 
# выдающимися показателями. Как всю таблицу, так и отдельные группы изучают, сортируя строки по какому-либо столбцу. 

# В pandas для этого есть метод sort_values(). У sort_values() для датафрейма два параметра:
    
    # by='имя столбца' — имя столбца, по которому нужно сортировать;
    # ascending — порядок сортировки. По умолчанию для него установлено значение True. 
    # Для сортировки по убыванию установите значение False.
    
# Если же метод sort_values() применяется к столбцу датафрейма (объекту Series), то параметр by отсутствует 
# (это логично, так как столбец уже один). Среди экзопланет интересны близкие по размерам к Земле. 
# Отсортируем список по радиусу в порядке возрастания. Тогда в начале таблицы окажутся самые маленькие.

# print(exoplanet.sort_values(by='radius').head(30))

# Теперь получим список экзопланет с радиусом меньше земного. Для этого послужит логическая индексация:

# print(exoplanet[exoplanet['radius'] < 1]) 

# Но и этот список такой длинный, что изучать его лучше по частям. 
# Экзопланеты, близкие по размерам к Земле, за последнее десятилетие открывали нередко. 
# Можно изучать список открытых за каждый год. Например, для 2014 года:

# print(exoplanet[exoplanet['discovered'] == 2014]) 

# Найдём среди планет те, что одновременно:

#  открыты в 2014,
#  меньше Земли.

# Для этого применим последовательную фильтрацию:

# exo_small_14 = exoplanet[exoplanet['radius'] < 1]
# exo_small_14 = exo_small_14[exo_small_14['discovered'] == 2014]
# print(exo_small_14)


# Отсортируем результат в порядке убывания радиуса.

# print(exo_small_14.sort_values(by='radius', ascending=False)) 

# Вы уже заметили, что sort_values(), как и другие методы pandas, возвращают новый объект в качестве результата. 
# Поэтому для дальнейшего использования отсортированного датафрейма вам придётся сохранить результат sort_values() в 
# переменную, например, ту же самую:

# exo_small_14 = exo_small_14.sort_values(by='radius', ascending=False) 


# ## Яндекс Музыка: сортировка данных

# In[4]:


# 1. У похожей на Солнце звезды телескоп Kepler открыл похожую на Землю планету. А вы нашли в данных Яндекс Музыки меломана 
# с уникальными данными. Он за день послушал больше 50 композиций. Получите таблицу с прослушанными им треками.
# Для этого запросите из структуры данных df строки, отвечающие сразу двум условиям:

#    Значение в столбце 'user_id' должно быть равно значению переменной search_id.
#   Время прослушивания, значение в столбце 'total_play_seconds', не должно равняться 0.

# Сохраните результат в переменной music_user, выводить её значение на экран не нужно.

# Подсказка: Задайте условия через логическую индексацию. Объедините их с помощью последовательной фильтрации.

# music_user = df.loc[df.loc[:,'user_id'] == search_id][df.loc[:,'total_play_seconds'] != 0]
# print(music_user)

# 2. Сгруппируйте данные таблицы music_user по столбцу 'genre_name' и получите сумму значений столбца 'total_play_seconds'. 
# Сохраните результат в переменной sum_music_user и выведите её значение на экран.

# sum_music_user = music_user.groupby('genre_name')['total_play_seconds'].sum()
# print(sum_music_user)


# 3. Предпочтения меломана начинают проявляться. Но, возможно, длительность композиций от жанра к жанру сильно различается. 
# Важно знать, сколько треков каждого жанра он включил. Сгруппируйте данные по столбцу genre_name и посчитайте значения 
# в столбце genre_name. Сохраните результат в переменной count_music_user и выведите её значение на экран.
# Вывод на экран из предыдущего задания закомментируйте.

# count_music_user = music_user.groupby('genre_name')['genre_name'].count()
# print(count_music_user)


# 4. Чтобы предпочтения были видны сразу, самые крупные значения нужно расположить сверху.
# Отсортируйте данные в группировке sum_music_user по убыванию. Когда применяете метод sort_values() к Series с единственным 
# столбцом, аргумент by указывать не нужно — только порядок сортировки. Сохраните результат в переменной final_sum и 
# выведите её значение на экран. Вывод на экран из предыдущего задания закомментируйте.

# final_sum = sum_music_user.sort_values(ascending=False)
# print(final_sum)


# ## Описательная статистика

# In[5]:


# Меломан оказался таким же последовательным поклонником определённых жанров, как и те, кому хватает пяти композиций в день 
# (исследования на остальных меломанах это подтверждают). Значит, по своим вкусам и поведению аудитория однородна, 
# оценивать её «вовлечённость» можно показателями «середнячков». Чтобы представить себе среднестатистического пользователя, 
# понадобится описательная статистика. Из её показателей для количественного описания данных в этой простой задаче нужны 
# четыре меры — максимум, минимум, медиана и среднее. Наибольшее и наименьшее обычно вычисляют только по одному признаку. 
# Например, можно получить минимальное и максимальное значение количества прослушанных секунд композиции 
# (столбец total_play_seconds). Для поиска максимума вызывают метод max():

# print(df['total_play_seconds'].max()) 

# Самый длинный трек звучал почти полтора часа. Интересно, какую композицию слушали так долго.
# Запросим из df строку с максимальным значением с помощью логической индексации с условием 
# df['total_play_seconds'] == df['total_play_seconds'].max():

# print(df[df['total_play_seconds'] == df['total_play_seconds'].max()]) 


# Минимальное значение — самый короткий трек — ищут методом min(). Понятно, что в Яндекс Музыке показателей меньше нуля не 
# бывает: если пользователь пролистнул трек, значение 'total_play_seconds' равно 0. 
# Вам интересно установить композиции, которые слушали хоть и недолго, но не пропустили сразу.
# Создадим выборку без пропущенных треков и найдём в ней минимальное значение:

# df_drop_null = df[df['total_play_seconds'] != 0] 
# print(df_drop_null['total_play_seconds'].min()) 


# Получим названия композиций, которые пропустили быстрее всего. Как и при поиске максимума, поможет логическая индексация:

# print(df_drop_null[df_drop_null['total_play_seconds'] == df_drop_null['total_play_seconds'].min()])


# Результат вывода — три трека, которые соответствуют минимальному значению. На основе полученных данных можно сделать вывод, 
# что время прослушивания треков находится в диапазоне от 0,000794 до 4707,422018 секунды, не включая пропущенные.
# Это знание пригодится, чтобы разобраться с медианой и средним арифметическим. Среднее и медиана оценивают значения в 
# центре выборки. Если тех, кто слушает долго, столько же, сколько тех, кто слушает мало, — среднее подойдёт. 
# Но когда есть оторванные от основной массы лидеры, слушающие музыку по 8 часов, их результаты сильно смещают значение 
# среднего вверх. Вот почему оценивать предпочтения широкого круга потребителей лучше медианой.

# В геометрии медиана делит треугольник на два равных по площади. В статистике она делит выборку пополам: 
# в одной половине значения меньше медианного, в другой — больше. Логично, что для определения медианы список обязательно 
# должен быть отсортирован — либо по возрастанию, либо по убыванию.
# Когда количество значений нечётное, медиана будет равна тому значению, которое оказалось ровно посередине отсортированного 
# набора. Если же количество данных чётное, то медиана рассчитывается как среднее арифметическое двух соседних чисел в 
# середине набора. Для примера посмотрите на 5 последних строк в таблице. 
# Обратитесь к столбцу total_play_seconds и отсортируйте его:

# df_stat_1 = df.tail()
# print(df_stat_1['total_play_seconds'].sort_values()) 


# Значение в середине равно 109 — это и будет медиана. Далее требуется получить 4 последние строки в таблице и обратиться к 
# столбцу total_play_seconds:

# df_stat = df.tail(4)
# print(df_stat['total_play_seconds'].sort_values()) 

# Можно взять два значения в середине — 26,127 и 220,551837. Подсчёт среднего: (26.127 + 220.551837) / 2 = 123.3394185 

# В pandas есть метод median(), который считает медиану. 
# По аналогии с min() и max() его можно применять ко всей таблице, к отдельному столбцу или к сгруппированным данным.
# Сравнение результатов с теми, которые были получены вручную:

# print(df_stat['total_play_seconds'].median()) - 123.3394185 


# Теперь применение медианы ко всем значениям времени прослушивания в нашей таблице, кроме нулевых:

# df_drop_null = df[df['total_play_seconds'] != 0]
# print(df_drop_null['total_play_seconds'].median()) 

# Чтобы убедиться, что лидеры действительно смещают средний показатель вверх, найдите 
# среднее арифметическое всех этих значений методом mean():

# print(df_drop_null['total_play_seconds'].mean()) 


# ## Яндекс Музыка: описательная статистика

# In[6]:


# 1. Получите таблицу с композициями самого популярного жанра — pop. 
# Затем исключите пропущенные треки — которые слушали 0 секунд. Сохраните результат в переменной pop_music.

# pop_music = df.loc[df.loc[:,'genre_name'] == 'pop'][df.loc[:,'total_play_seconds'] != 0]

# 2. Найдите максимальное время прослушивания песни в жанре pop. 
# Сохраните результат в переменной pop_music_max_total_play и выведите её значение на экран.

# pop_music_max_total_play = pop_music['total_play_seconds'].max()
# print(pop_music_max_total_play)

# 3. Получите из таблицы pop_music строку с максимальным временем прослушивания. 
# Результат сохраните в переменной pop_music_max_info и выведите на экран. Закомментируйте вывод результата предыдущей задачи.

# pop_music_max_info = pop_music[pop_music['total_play_seconds'] == pop_music['total_play_seconds'].max()]
# print(pop_music_max_info)

# 4. and 5. - the same with min() values.

# 6. Рассчитайте медиану времени прослушивания произведений жанра pop. Сохраните результат в переменной pop_music_median и 
# выведите на экран. Вывод результата предыдущей задачи закомментируйте.

# pop_music_median = pop_music['total_play_seconds'].median()
# print(pop_music_median)

# 7. Рассчитайте среднее арифметическое времени прослушивания произведений жанра pop. 
# Сохраните результат в переменной pop_music_mean и выведите на экран. Вывод результата предыдущей задачи закомментируйте.

# pop_music_mean = pop_music['total_play_seconds'].mean()
# print(pop_music_mean)


# ## Яндекс Музыка: решение кейса и оформление результатов

# In[8]:


# 1. Рассчитайте метрику engagement после проведения эксперимента для всего набора данных. 
# Сохраните полученный результат в переменной current_engagement и выведите на экран.

# current_engagement = df.groupby('user_id').sum().median()
# print(current_engagement)

# 3. Получите выборку прослушанных композиций в жанре рок, время воспроизведения которых не равно нулю, и 
# сохраните её в переменной genre_rock. Получите максимальное и минимальное значения времени прослушивания, 
# сохраните соответственно в переменных genre_rock_max и genre_rock_min, выведите на экран со строками:

#   'Максимальное время прослушивания в жанре рок равно:'
#   'Минимальное время прослушивания в жанре рок равно:'

# genre_rock = df.loc[df.loc[:,'genre_name'] == 'rock'][df.loc[:,'total_play_seconds'] != 0]
# genre_rock_max = genre_rock['total_play_seconds'].max()
# genre_rock_min = genre_rock['total_play_seconds'].min()
# print('Максимальное время прослушивания в жанре рок равно:', genre_rock_max)
# print('Минимальное время прослушивания в жанре рок равно:', genre_rock_min)


# 4. Соберите результаты исследования в таблицу research_genres_result, которую нужно создать конструктором DataFrame(). 
# Его аргумент data — список с данными, аргумент columns — список названий столбцов. Выведите полученную таблицу на экран.

# research_genres_result = pd.DataFrame(data = data, columns=columns)
# print(research_genres_result)


# [Предобработка данных](#dataprep)

# In[ ]:




